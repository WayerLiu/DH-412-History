{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0      book chapter  \\\n",
      "0           0  analects      學而   \n",
      "1           1  analects      為政   \n",
      "2           2  analects      八佾   \n",
      "3           3  analects      里仁   \n",
      "4           4  analects     公冶長   \n",
      "\n",
      "                                             content  \n",
      "0  子曰：「學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？」\\n\\n有...  \n",
      "1  子曰：「為政以德，譬如北辰，居其所而眾星共之。」\\n\\n子曰：「詩三百，一言以蔽之，曰『思無...  \n",
      "2  孔子謂季氏：「八佾舞於庭，是可忍也，孰不可忍也？」\\n\\n三家者以雍徹。子曰：「『相維辟公，...  \n",
      "3  子曰：「里仁為美。擇不處仁，焉得知？」\\n\\n子曰：「不仁者不可以久處約，不可以長處樂。仁者...  \n",
      "4  子謂公冶長，「可妻也。雖在縲絏之中，非其罪也」。以其子妻之。\\n\\n子謂南容，「邦有道，不廢...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset.csv file and store it in a dataframe\n",
    "data_fan = pd.read_csv('dataset.csv')\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "print(data_fan.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify Chinese characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'content' into separate sentences using '\\n\\n', then explode into new rows\n",
    "data_fan = data_fan.assign(sentences=data_fan['content'].str.split('\\n\\n')).explode('sentences')\n",
    "\n",
    "# Create 'book', 'chapter', 'sentences' structure\n",
    "data_fan = data_fan[['book', 'chapter', 'sentences']]\n",
    "\n",
    "# Show the modified DataFrame\n",
    "data_fan.head()\n",
    "\n",
    "# set new index\n",
    "data_fan = data_fan.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>子曰：「學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？」</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>有子曰：「其為人也孝弟，而好犯上者，鮮矣；不好犯上，而好作亂者，未之有也。君子務本，本立而道...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>子曰：「巧言令色，鮮矣仁！」</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>曾子曰：「吾日三省吾身：為人謀而不忠乎？與朋友交而不信乎？傳不習乎？」</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>子曰：「道千乘之國：敬事而信，節用而愛人，使民以時。」</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book chapter                                          sentences\n",
       "0  analects      學而          子曰：「學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？」\n",
       "1  analects      學而  有子曰：「其為人也孝弟，而好犯上者，鮮矣；不好犯上，而好作亂者，未之有也。君子務本，本立而道...\n",
       "2  analects      學而                                     子曰：「巧言令色，鮮矣仁！」\n",
       "3  analects      學而                曾子曰：「吾日三省吾身：為人謀而不忠乎？與朋友交而不信乎？傳不習乎？」\n",
       "4  analects      學而                        子曰：「道千乘之國：敬事而信，節用而愛人，使民以時。」"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>有子曰：“其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>子曰：“巧言令色，鲜矣仁！”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>曾子曰：“吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>子曰：“道千乘之国：敬事而信，节用而爱人，使民以时。”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book chapter                                          sentences\n",
       "0  analects      学而          子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？”\n",
       "0  analects      学而  有子曰：“其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道...\n",
       "0  analects      学而                                     子曰：“巧言令色，鲜矣仁！”\n",
       "0  analects      学而                曾子曰：“吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？”\n",
       "0  analects      学而                        子曰：“道千乘之国：敬事而信，节用而爱人，使民以时。”"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zhconv import convert\n",
    "# Convert the content to Simplified Chinese\n",
    "data['chapter'] = data['chapter'].apply(lambda x: convert(x, 'zh-cn'))\n",
    "data['content'] = data['content'].apply(lambda x: convert(x, 'zh-cn'))\n",
    "\n",
    "# Split the 'content' into separate sentences using '\\n\\n', then explode into new rows\n",
    "data = data.assign(sentences=data['content'].str.split('\\n\\n')).explode('sentences')\n",
    "\n",
    "# Create 'book', 'chapter', 'sentences' structure\n",
    "data = data[['book', 'chapter', 'sentences']]\n",
    "\n",
    "# Show the modified DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Ren occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>sentences</th>\n",
       "      <th>contain_ren</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>有子曰：“其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>子曰：“巧言令色，鲜矣仁！”</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>曾子曰：“吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>子曰：“道千乘之国：敬事而信，节用而爱人，使民以时。”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book chapter                                          sentences  \\\n",
       "0  analects      学而          子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？”   \n",
       "0  analects      学而  有子曰：“其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道...   \n",
       "0  analects      学而                                     子曰：“巧言令色，鲜矣仁！”   \n",
       "0  analects      学而                曾子曰：“吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？”   \n",
       "0  analects      学而                        子曰：“道千乘之国：敬事而信，节用而爱人，使民以时。”   \n",
       "\n",
       "   contain_ren  \n",
       "0            0  \n",
       "0            1  \n",
       "0            1  \n",
       "0            0  \n",
       "0            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column 'contain_ren' to check if the sentence contains the character '仁'\n",
    "data['contain_ren'] = data['sentences'].apply(lambda x: 1 if '仁' in x else 0)\n",
    "\n",
    "# Show the modified DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the DataFrame is 12604\n",
      "The number of sentences that contain the character \"仁\" is 921\n"
     ]
    }
   ],
   "source": [
    "print(f'The length of the DataFrame is {len(data)}', \n",
    "      f'The number of sentences that contain the character \"仁\" is {len(data[data[\"contain_ren\"] == 1])}', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv('modified_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add time attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>sentences</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>子曰：「學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？」</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>有子曰：「其為人也孝弟，而好犯上者，鮮矣；不好犯上，而好作亂者，未之有也。君子務本，本立而道...</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>子曰：「巧言令色，鮮矣仁！」</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>曾子曰：「吾日三省吾身：為人謀而不忠乎？與朋友交而不信乎？傳不習乎？」</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>子曰：「道千乘之國：敬事而信，節用而愛人，使民以時。」</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book chapter                                          sentences Start  \\\n",
       "0  analects      學而          子曰：「學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？」  -480   \n",
       "1  analects      學而  有子曰：「其為人也孝弟，而好犯上者，鮮矣；不好犯上，而好作亂者，未之有也。君子務本，本立而道...  -480   \n",
       "2  analects      學而                                     子曰：「巧言令色，鮮矣仁！」  -480   \n",
       "3  analects      學而                曾子曰：「吾日三省吾身：為人謀而不忠乎？與朋友交而不信乎？傳不習乎？」  -480   \n",
       "4  analects      學而                        子曰：「道千乘之國：敬事而信，節用而愛人，使民以時。」  -480   \n",
       "\n",
       "    End average  \n",
       "0  -350  -415.0  \n",
       "1  -350  -415.0  \n",
       "2  -350  -415.0  \n",
       "3  -350  -415.0  \n",
       "4  -350  -415.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_bookname = list(data_fan[\"book\"].unique())\n",
    "list_book_time = [\n",
    "    ['analects',-480, -350],\n",
    "    ['mengzi',  -340, -250],\n",
    "    ['liji',    -475, -221],\n",
    "    ['xunzi',   -475, -221],\n",
    "    ['xiao-jing',-475, -221],\n",
    "    ['shuo-yuan',-206, 9],\n",
    "    ['chun-qiu-fan-lu',-206, 9],\n",
    "    ['han-shi-wai-zhuan',-180, -120],\n",
    "    ['da-dai-li-ji',100, 200],\n",
    "    ['bai-hu-tong',79, 92],\n",
    "    ['xin-shu', -206, 9],\n",
    "    ['xin-xu',-206, 9],\n",
    "    ['yangzi-fayan',-33, 18],\n",
    "    ['zhong-lun',25, 220],\n",
    "    ['kongzi-jiayu',-206, 220],\n",
    "    ['qian-fu-lun',102, 167],\n",
    "    ['lunheng', 80, 80],\n",
    "    ['taixuanjing',-33, 18],\n",
    "    ['fengsutongyi',190, 200],\n",
    "    ['kongcongzi',25, 265],\n",
    "    ['shenjian',196, 220],\n",
    "    ['zhong-jing',100,166],\n",
    "    ['su-shu',-250,-186],\n",
    "    ['xinyu',-196, -196],\n",
    "    ['duduan',167, 258],\n",
    "    ['caizhong-langji', 152, 192]\n",
    "    ]\n",
    "data_fan[\"Start\"] = None\n",
    "data_fan[\"End\"] = None\n",
    "num = list_bookname.index(data_fan.iloc[0][\"book\"])\n",
    "for i in range(data_fan.shape[0]):\n",
    "    num = list_bookname.index(data_fan.iloc[i][\"book\"])\n",
    "    list_book_time[num]\n",
    "    data_fan.loc[i, 'Start'] = list_book_time[num][1]\n",
    "    data_fan.loc[i, 'End'] = list_book_time[num][2]\n",
    "data_fan['average'] = (data_fan['Start'] + data_fan['End'])/2\n",
    "\n",
    "data_fan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert-ancient-chiense to do word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at Jihuai/bert-ancient-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jihuai/bert-ancient-chinese\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"Jihuai/bert-ancient-chinese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'子曰：「學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？」'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fan['sentences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2094, 3288, 8038, 519, 2119, 5445, 3229, 5424, 722, 8024, 679, 771, 6303, 725, 8043, 3300, 3301, 5632, 6895, 3175, 889, 8024, 679, 771, 3556, 725, 8043, 782, 679, 4761, 5445, 679, 21628, 8024, 679, 771, 1409, 2094, 725, 8043, 520, 102]\n",
      "[CLS] 子 曰 ： 「 學 而 時 習 之 ， 不 亦 說 乎 ？ 有 朋 自 遠 方 來 ， 不 亦 樂 乎 ？ 人 不 知 而 不 慍 ， 不 亦 君 子 乎 ？ 」 [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = data_fan['sentences'][0]\n",
    "encoded_input = tokenizer.encode(text)\n",
    "print(encoded_input)\n",
    "encode_tensor = (torch.tensor([encoded_input]))\n",
    "decoded_string = tokenizer.decode(encoded_input)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roberta-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForTokenClassification\n",
    "rob_tokenizer = AutoTokenizer.from_pretrained(\"KoichiYasuoka/roberta-classical-chinese-base-sentence-segmentation\")\n",
    "rob_model = AutoModelForTokenClassification.from_pretrained(\"KoichiYasuoka/roberta-classical-chinese-base-sentence-segmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子曰。學而時習之。不亦説乎。有朋自遠方來。不亦樂乎。人不知而不慍。不亦君子乎。\n"
     ]
    }
   ],
   "source": [
    "s = \"子曰學而時習之不亦説乎有朋自遠方來不亦樂乎人不知而不慍不亦君子乎\"\n",
    "p = [rob_model.config.id2label[q] for q in torch.argmax(rob_model(rob_tokenizer.encode(s,return_tensors=\"pt\"))[\"logits\"],dim=2)[0].tolist()[1:-1]]\n",
    "print(\"\".join(c+\"。\" if q==\"E\" or q==\"S\" else c for c,q in zip(s,p)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m example \u001b[38;5;241m=\u001b[39m data_fan[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m \u001b[43mrob_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(tokenized_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m tokens\n",
      "File \u001b[1;32md:\\anaconda\\envs\\mnlpa1\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2803\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2801\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2802\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2803\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\mnlpa1\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2909\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2890\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2891\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2906\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2907\u001b[0m     )\n\u001b[0;32m   2908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2909\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2910\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2911\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   2912\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2913\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2914\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2915\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2916\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2917\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2918\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2919\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2920\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2921\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2922\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2923\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2924\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2925\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2926\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2927\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2928\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\envs\\mnlpa1\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2982\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2972\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2973\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2974\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2975\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2979\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2980\u001b[0m )\n\u001b[1;32m-> 2982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   2983\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2984\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   2985\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2986\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2987\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2988\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2989\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2990\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2991\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2992\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2993\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2994\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2995\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2996\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2997\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2998\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2999\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3000\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3001\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda\\envs\\mnlpa1\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[0;32m    575\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[1;32m--> 576\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m    577\u001b[0m         batched_input,\n\u001b[0;32m    578\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m    579\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    580\u001b[0m         padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m    581\u001b[0m         truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m    582\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    583\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m    584\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    585\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    586\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    587\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    588\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    589\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    590\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    591\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    592\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    594\u001b[0m     )\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\mnlpa1\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    502\u001b[0m )\n\u001b[1;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[0;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[0;32m    528\u001b[0m ]\n",
      "\u001b[1;31mTypeError\u001b[0m: PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]"
     ]
    }
   ],
   "source": [
    "example = data_fan['sentences'][0]\n",
    "tokenized_input = rob_tokenizer(example, is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 文档-词频矩阵\u001b[39;00m\n\u001b[1;32m     18\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(tokenizer\u001b[38;5;241m=\u001b[39mchinese_tokenizer, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)  \u001b[38;5;66;03m# 可调整max_features根据数据大小\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m dtm \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# LDA模型\u001b[39;00m\n\u001b[1;32m     22\u001b[0m lda \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 可调整n_components为主题数\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m, in \u001b[0;36mchinese_tokenizer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchinese_tokenizer\u001b[39m(text):\n\u001b[0;32m----> 7\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mjieba\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/jieba/__init__.py:357\u001b[0m, in \u001b[0;36mTokenizer.lcut\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlcut\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcut\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/jieba/__init__.py:325\u001b[0m, in \u001b[0;36mTokenizer.cut\u001b[0;34m(self, sentence, cut_all, HMM, use_paddle)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m re_han\u001b[38;5;241m.\u001b[39mmatch(blk):\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m cut_block(blk):\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m word\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/jieba/__init__.py:252\u001b[0m, in \u001b[0;36mTokenizer.__cut_DAG\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    250\u001b[0m DAG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_DAG(sentence)\n\u001b[1;32m    251\u001b[0m route \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDAG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    254\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/jieba/__init__.py:177\u001b[0m, in \u001b[0;36mTokenizer.calc\u001b[0;34m(self, sentence, DAG, route)\u001b[0m\n\u001b[1;32m    175\u001b[0m logtotal \u001b[38;5;241m=\u001b[39m log(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m xrange(N \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     route[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((log(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFREQ\u001b[38;5;241m.\u001b[39mget(sentence[idx:x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m    178\u001b[0m                       logtotal \u001b[38;5;241m+\u001b[39m route[x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m DAG[idx])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 数据预处理：分词和去除停用词\n",
    "def chinese_tokenizer(text):\n",
    "    tokens = jieba.lcut(text)\n",
    "    return [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "\n",
    "# 加载数据\n",
    "# 假设 data['sentences'] 是你要分析的文本数据列\n",
    "corpus = data['sentences'].tolist()\n",
    "\n",
    "# 停用词列表，需要你根据实际情况扩展\n",
    "stop_words = set(['之', '乎', '者', '也', '曰', '子', ])\n",
    "\n",
    "# 文档-词频矩阵\n",
    "vectorizer = CountVectorizer(tokenizer=chinese_tokenizer, max_features=1000)  # 可调整max_features根据数据大小\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# LDA模型\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)  # 可调整n_components为主题数\n",
    "lda.fit(dtm)\n",
    "\n",
    "# 查看主题\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# 显示主题\n",
    "no_top_words = 10\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)\n",
    "\n",
    "# Visualize the topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, dtm, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 785, 5442, 4263, 782, 8024, 782, 2608, 4263, 722, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_tokenizer('仁者爱人，人恒爱之。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:44<00:00, 9.90MB/s] \n",
      "/Users/wayer/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8366, -0.4070,  0.5772,  ...,  1.3666, -0.6914, -0.6586],\n",
      "         [-0.3661, -0.2313,  0.4520,  ...,  0.5482, -0.5783, -0.2224],\n",
      "         [-0.3812,  0.3538,  0.3265,  ...,  0.3139, -0.4686, -0.6361],\n",
      "         ...,\n",
      "         [ 0.4973,  0.2635, -0.1911,  ...,  0.7610,  0.1643, -0.0059],\n",
      "         [-0.2748,  0.4787,  0.7579,  ...,  0.6090, -0.5992, -1.0056],\n",
      "         [-0.8366, -0.4070,  0.5772,  ...,  1.3666, -0.6914, -0.6586]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SIKU-BERT/sikubert\")\n",
    "model = AutoModel.from_pretrained(\"SIKU-BERT/sikubert\")\n",
    "# 示例文本\n",
    "text = \"你好，世界！\"\n",
    "\n",
    "# 使用分词器处理文本\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# 使用模型获取文本的特征表示\n",
    "with torch.no_grad():\n",
    "    output = model(**encoded_input)\n",
    "\n",
    "# 输出结果，例如输出第一个隐藏状态的向量\n",
    "print(output.last_hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           sentences  \\\n",
      "0          子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？”   \n",
      "0  有子曰：“其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道...   \n",
      "0                                     子曰：“巧言令色，鲜矣仁！”   \n",
      "0                曾子曰：“吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？”   \n",
      "0                        子曰：“道千乘之国：敬事而信，节用而爱人，使民以时。”   \n",
      "\n",
      "                                            unigrams  \\\n",
      "0  [子, 曰, 学, 而, 时, 习, 之, 亦, 说, 乎, 朋, 自, 远, 方, 来, ...   \n",
      "0  [子, 曰, 其, 为, 人, 也, 孝, 弟, 而, 好, 犯, 上, 者, 鲜, 矣, ...   \n",
      "0                        [子, 曰, 巧, 言, 令, 色, 鲜, 矣, 仁]   \n",
      "0  [曾, 子, 曰, 吾, 日, 三, 省, 吾, 身, 为, 人, 谋, 而, 忠, 乎, ...   \n",
      "0  [子, 曰, 道, 千, 乘, 之, 国, 敬, 事, 而, 信, 节, 用, 而, 爱, ...   \n",
      "\n",
      "                                             bigrams  \n",
      "0  [子曰, 曰学, 学而, 而时, 时习, 习之, 之亦, 亦说, 说乎, 乎朋, 朋自, 自...  \n",
      "0  [子曰, 曰其, 其为, 为人, 人也, 也孝, 孝弟, 弟而, 而好, 好犯, 犯上, 上...  \n",
      "0                   [子曰, 曰巧, 巧言, 言令, 令色, 色鲜, 鲜矣, 矣仁]  \n",
      "0  [曾子, 子曰, 曰吾, 吾日, 日三, 三省, 省吾, 吾身, 身为, 为人, 人谋, 谋...  \n",
      "0  [子曰, 曰道, 道千, 千乘, 乘之, 之国, 国敬, 敬事, 事而, 而信, 信节, 节...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 中文标点和英文标点\n",
    "    punctuations = r'[，。！？、；：“”‘’（）《》【】『』「」\\s+\\.\\!\\/_,$%^*()+\\\"\\']+|[+——！，;:。？、~@#￥%……&*（）]+'\n",
    "    text = re.sub(punctuations, '', text)\n",
    "    return text\n",
    "\n",
    "# 停用词列表（示例，可扩展）\n",
    "stop_words = set(['的', '是', '在', '有', '和', '不'])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ''.join([char for char in text if char not in stop_words])\n",
    "\n",
    "def generate_ngrams(text, n=1):\n",
    "    text = clean_text(text)  # 清除标点\n",
    "    text = remove_stopwords(text)  # 去除停用词\n",
    "    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "# 应用到DataFrame中\n",
    "data['unigrams'] = data['sentences'].apply(lambda x: generate_ngrams(x, n=1))\n",
    "data['bigrams'] = data['sentences'].apply(lambda x: generate_ngrams(x, n=2))\n",
    "\n",
    "# 展示部分结果\n",
    "print(data[['sentences', 'unigrams', 'bigrams']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrams: [('之', 50709), ('也', 30599), ('而', 22435), ('以', 21000), ('子', 17866), ('者', 17428), ('曰', 17223), ('其', 16821), ('人', 14860), ('于', 13304)]\n",
      "Top 10 bigrams: [('子曰', 2924), ('孔子', 2578), ('天下', 2373), ('君子', 2201), ('人之', 1988), ('所以', 1933), ('以为', 1854), ('之所', 1636), ('子之', 1572), ('诸侯', 1466)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 合并所有n-gram列表并计算频率\n",
    "def get_ngram_frequencies(column):\n",
    "    ngram_list = data[column].sum()  # 将所有行的n-gram列表合并\n",
    "    ngram_counts = Counter(ngram_list)  # 计算每个n-gram的频率\n",
    "    # 返回按频率降序排列的n-gram和它们的计数\n",
    "    return sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 获取unigram和bigram的频率\n",
    "unigram_frequencies = get_ngram_frequencies('unigrams')\n",
    "bigram_frequencies = get_ngram_frequencies('bigrams')\n",
    "\n",
    "# 打印前10个最频繁的unigrams和bigrams\n",
    "print(\"Top 10 unigrams:\", unigram_frequencies[:10])\n",
    "print(\"Top 10 bigrams:\", bigram_frequencies[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('之', 50709),\n",
       " ('也', 30599),\n",
       " ('而', 22435),\n",
       " ('以', 21000),\n",
       " ('子', 17866),\n",
       " ('者', 17428),\n",
       " ('曰', 17223),\n",
       " ('其', 16821),\n",
       " ('人', 14860),\n",
       " ('于', 13304),\n",
       " ('为', 13089),\n",
       " ('则', 10001),\n",
       " ('天', 8324),\n",
       " ('君', 7557),\n",
       " ('无', 7555),\n",
       " ('故', 7457),\n",
       " ('所', 7288),\n",
       " ('王', 6226),\n",
       " ('夫', 6029),\n",
       " ('大', 5665),\n",
       " ('下', 5475),\n",
       " ('可', 5237),\n",
       " ('能', 5190),\n",
       " ('言', 5148),\n",
       " ('矣', 5138),\n",
       " ('何', 4957),\n",
       " ('公', 4709),\n",
       " ('行', 4668),\n",
       " ('知', 4634),\n",
       " ('乎', 4551),\n",
       " ('谓', 4496),\n",
       " ('与', 4222),\n",
       " ('道', 4084),\n",
       " ('一', 4071),\n",
       " ('三', 4039),\n",
       " ('得', 4034),\n",
       " ('此', 4024),\n",
       " ('事', 3862),\n",
       " ('国', 3828),\n",
       " ('非', 3820),\n",
       " ('民', 3799),\n",
       " ('如', 3798),\n",
       " ('见', 3689),\n",
       " ('然', 3664),\n",
       " ('生', 3603),\n",
       " ('礼', 3560),\n",
       " ('臣', 3535),\n",
       " ('后', 3480),\n",
       " ('上', 3336),\n",
       " ('必', 3164),\n",
       " ('至', 3056),\n",
       " ('使', 2973),\n",
       " ('自', 2966),\n",
       " ('中', 2897),\n",
       " ('时', 2881),\n",
       " ('明', 2862),\n",
       " ('文', 2811),\n",
       " ('孔', 2770),\n",
       " ('死', 2756),\n",
       " ('日', 2731),\n",
       " ('德', 2729),\n",
       " ('五', 2637),\n",
       " ('相', 2570),\n",
       " ('焉', 2542),\n",
       " ('义', 2536),\n",
       " ('诸', 2444),\n",
       " ('善', 2437),\n",
       " ('十', 2432),\n",
       " ('用', 2406),\n",
       " ('若', 2374),\n",
       " ('世', 2366),\n",
       " ('士', 2352),\n",
       " ('成', 2348),\n",
       " ('今', 2347),\n",
       " ('吾', 2345),\n",
       " ('侯', 2345),\n",
       " ('地', 2341),\n",
       " ('问', 2312),\n",
       " ('亦', 2279),\n",
       " ('闻', 2262),\n",
       " ('皆', 2256),\n",
       " ('未', 2222),\n",
       " ('二', 2222),\n",
       " ('欲', 2216),\n",
       " ('贤', 2157),\n",
       " ('百', 2138),\n",
       " ('命', 2138),\n",
       " ('食', 2120),\n",
       " ('出', 2108),\n",
       " ('将', 2092),\n",
       " ('圣', 2090),\n",
       " ('先', 2071),\n",
       " ('心', 2058),\n",
       " ('乐', 2016),\n",
       " ('四', 1955),\n",
       " ('父', 1907),\n",
       " ('乃', 1886),\n",
       " ('恶', 1876),\n",
       " ('治', 1859),\n",
       " ('物', 1853),\n",
       " ('帝', 1844),\n",
       " ('从', 1836),\n",
       " ('正', 1794),\n",
       " ('周', 1780),\n",
       " ('或', 1779),\n",
       " ('身', 1773),\n",
       " ('气', 1765),\n",
       " ('小', 1721),\n",
       " ('主', 1717),\n",
       " ('名', 1709),\n",
       " ('太', 1697),\n",
       " ('立', 1691),\n",
       " ('齐', 1687),\n",
       " ('安', 1670),\n",
       " ('神', 1627),\n",
       " ('仁', 1595),\n",
       " ('书', 1595),\n",
       " ('足', 1593),\n",
       " ('我', 1587),\n",
       " ('长', 1586),\n",
       " ('方', 1580),\n",
       " ('犹', 1560),\n",
       " ('及', 1544),\n",
       " ('月', 1536),\n",
       " ('虽', 1535),\n",
       " ('亲', 1530),\n",
       " ('法', 1512),\n",
       " ('服', 1507),\n",
       " ('年', 1505),\n",
       " ('当', 1503),\n",
       " ('阳', 1497),\n",
       " ('已', 1494),\n",
       " ('同', 1491),\n",
       " ('亡', 1476),\n",
       " ('哉', 1467),\n",
       " ('功', 1463),\n",
       " ('莫', 1452),\n",
       " ('入', 1445),\n",
       " ('政', 1433),\n",
       " ('失', 1422),\n",
       " ('贵', 1412),\n",
       " ('过', 1381),\n",
       " ('家', 1361),\n",
       " ('高', 1324),\n",
       " ('多', 1301),\n",
       " ('乱', 1297),\n",
       " ('好', 1289),\n",
       " ('作', 1289),\n",
       " ('令', 1287),\n",
       " ('氏', 1252),\n",
       " ('诗', 1240),\n",
       " ('受', 1234),\n",
       " ('居', 1228),\n",
       " ('信', 1227),\n",
       " ('又', 1226),\n",
       " ('学', 1225),\n",
       " ('易', 1220),\n",
       " ('山', 1216),\n",
       " ('武', 1214),\n",
       " ('内', 1210),\n",
       " ('实', 1206),\n",
       " ('复', 1198),\n",
       " ('去', 1196),\n",
       " ('祭', 1191),\n",
       " ('取', 1190),\n",
       " ('水', 1183),\n",
       " ('万', 1183),\n",
       " ('杀', 1161),\n",
       " ('云', 1147),\n",
       " ('敢', 1147),\n",
       " ('六', 1146),\n",
       " ('反', 1144),\n",
       " ('丧', 1136),\n",
       " ('数', 1136),\n",
       " ('称', 1131),\n",
       " ('敬', 1123),\n",
       " ('朝', 1119),\n",
       " ('说', 1112),\n",
       " ('位', 1111),\n",
       " ('孝', 1107),\n",
       " ('利', 1107),\n",
       " ('教', 1102),\n",
       " ('春', 1097),\n",
       " ('众', 1096),\n",
       " ('师', 1084),\n",
       " ('门', 1078),\n",
       " ('异', 1075),\n",
       " ('远', 1073),\n",
       " ('古', 1073),\n",
       " ('母', 1069),\n",
       " ('重', 1066),\n",
       " ('难', 1062),\n",
       " ('外', 1052),\n",
       " ('由', 1051),\n",
       " ('阴', 1045),\n",
       " ('罪', 1037),\n",
       " ('夏', 1036),\n",
       " ('始', 1031),\n",
       " ('尊', 1028),\n",
       " ('对', 1027),\n",
       " ('平', 1027),\n",
       " ('官', 1021),\n",
       " ('求', 1013),\n",
       " ('变', 1009),\n",
       " ('秋', 994),\n",
       " ('性', 989),\n",
       " ('马', 985),\n",
       " ('举', 981),\n",
       " ('九', 980),\n",
       " ('秦', 979),\n",
       " ('伯', 976),\n",
       " ('来', 975),\n",
       " ('终', 960),\n",
       " ('既', 957),\n",
       " ('次', 953),\n",
       " ('东', 953),\n",
       " ('辞', 951),\n",
       " ('里', 949),\n",
       " ('制', 948),\n",
       " ('衣', 947),\n",
       " ('且', 946),\n",
       " ('南', 943),\n",
       " ('思', 941),\n",
       " ('己', 929),\n",
       " ('志', 924),\n",
       " ('致', 923),\n",
       " ('楚', 912),\n",
       " ('本', 906),\n",
       " ('仲', 905),\n",
       " ('声', 900),\n",
       " ('忠', 897),\n",
       " ('常', 891),\n",
       " ('刑', 884),\n",
       " ('七', 883),\n",
       " ('守', 881),\n",
       " ('传', 868),\n",
       " ('遂', 864),\n",
       " ('力', 862),\n",
       " ('姓', 859),\n",
       " ('顺', 857),\n",
       " ('观', 854),\n",
       " ('鲁', 850),\n",
       " ('动', 850),\n",
       " ('尚', 846),\n",
       " ('土', 846),\n",
       " ('进', 845),\n",
       " ('通', 845),\n",
       " ('宜', 845),\n",
       " ('车', 839),\n",
       " ('意', 838),\n",
       " ('色', 823),\n",
       " ('听', 822),\n",
       " ('合', 822),\n",
       " ('独', 820),\n",
       " ('请', 819),\n",
       " ('告', 815),\n",
       " ('爱', 814),\n",
       " ('修', 813),\n",
       " ('祖', 809),\n",
       " ('庙', 805),\n",
       " ('宗', 801),\n",
       " ('皇', 801),\n",
       " ('弟', 797),\n",
       " ('论', 796),\n",
       " ('节', 794),\n",
       " ('少', 794),\n",
       " ('测', 794),\n",
       " ('木', 791),\n",
       " ('尽', 790),\n",
       " ('养', 788),\n",
       " ('应', 788),\n",
       " ('起', 782),\n",
       " ('耳', 781),\n",
       " ('化', 779),\n",
       " ('情', 773),\n",
       " ('归', 770),\n",
       " ('因', 770),\n",
       " ('前', 768),\n",
       " ('岁', 768),\n",
       " ('形', 767),\n",
       " ('孙', 764),\n",
       " ('西', 764),\n",
       " ('寡', 756),\n",
       " ('富', 752),\n",
       " ('弗', 751),\n",
       " ('兵', 747),\n",
       " ('类', 747),\n",
       " ('发', 746),\n",
       " ('女', 738),\n",
       " ('千', 735),\n",
       " ('疾', 733),\n",
       " ('益', 729),\n",
       " ('理', 729),\n",
       " ('孟', 728),\n",
       " ('司', 726),\n",
       " ('凡', 723),\n",
       " ('伐', 713),\n",
       " ('舜', 712),\n",
       " ('吏', 712),\n",
       " ('体', 711),\n",
       " ('龙', 710),\n",
       " ('雨', 708),\n",
       " ('北', 704),\n",
       " ('左', 698),\n",
       " ('处', 696),\n",
       " ('胜', 696),\n",
       " ('美', 692),\n",
       " ('止', 689),\n",
       " ('度', 682),\n",
       " ('白', 681),\n",
       " ('祸', 681),\n",
       " ('贱', 678),\n",
       " ('定', 675),\n",
       " ('老', 675),\n",
       " ('分', 671),\n",
       " ('象', 671),\n",
       " ('风', 670),\n",
       " ('害', 666),\n",
       " ('哀', 666),\n",
       " ('妇', 665),\n",
       " ('火', 664),\n",
       " ('御', 662),\n",
       " ('视', 660),\n",
       " ('尧', 656),\n",
       " ('八', 655),\n",
       " ('固', 653),\n",
       " ('卿', 653),\n",
       " ('路', 650),\n",
       " ('经', 649),\n",
       " ('岂', 646),\n",
       " ('兴', 642),\n",
       " ('田', 642),\n",
       " ('室', 640),\n",
       " ('晋', 639),\n",
       " ('金', 638),\n",
       " ('陈', 631),\n",
       " ('往', 628),\n",
       " ('徒', 627),\n",
       " ('黄', 626),\n",
       " ('鬼', 624),\n",
       " ('封', 623),\n",
       " ('任', 620),\n",
       " ('执', 619),\n",
       " ('斯', 618),\n",
       " ('乘', 615),\n",
       " ('精', 611),\n",
       " ('冠', 610),\n",
       " ('甚', 607),\n",
       " ('群', 606),\n",
       " ('右', 605),\n",
       " ('加', 600),\n",
       " ('语', 598),\n",
       " ('绝', 594),\n",
       " ('卒', 594),\n",
       " ('俗', 594),\n",
       " ('赐', 592),\n",
       " ('施', 590),\n",
       " ('近', 589),\n",
       " ('昔', 589),\n",
       " ('忧', 588),\n",
       " ('直', 585),\n",
       " ('比', 583),\n",
       " ('谷', 580),\n",
       " ('石', 579),\n",
       " ('诚', 578),\n",
       " ('察', 577),\n",
       " ('章', 577),\n",
       " ('流', 577),\n",
       " ('汉', 575),\n",
       " ('祀', 574),\n",
       " ('余', 573),\n",
       " ('儒', 571),\n",
       " ('尔', 570),\n",
       " ('各', 567),\n",
       " ('智', 567),\n",
       " ('危', 565),\n",
       " ('季', 563),\n",
       " ('怒', 562),\n",
       " ('宫', 556),\n",
       " ('尝', 553),\n",
       " ('禄', 552),\n",
       " ('间', 552),\n",
       " ('口', 550),\n",
       " ('史', 550),\n",
       " ('病', 550),\n",
       " ('夷', 547),\n",
       " ('卫', 546),\n",
       " ('诛', 544),\n",
       " ('社', 543),\n",
       " ('贡', 541),\n",
       " ('容', 539),\n",
       " ('福', 539),\n",
       " ('哭', 538),\n",
       " ('彼', 537),\n",
       " ('饮', 536),\n",
       " ('曾', 535),\n",
       " ('达', 532),\n",
       " ('目', 531),\n",
       " ('叔', 531),\n",
       " ('爵', 531),\n",
       " ('威', 529),\n",
       " ('存', 529),\n",
       " ('盛', 528),\n",
       " ('穷', 528),\n",
       " ('拜', 525),\n",
       " ('患', 524),\n",
       " ('邪', 524),\n",
       " ('伤', 524),\n",
       " ('桓', 522),\n",
       " ('极', 522),\n",
       " ('别', 521),\n",
       " ('微', 519),\n",
       " ('退', 516),\n",
       " ('即', 515),\n",
       " ('殷', 510),\n",
       " ('惟', 508),\n",
       " ('赵', 505),\n",
       " ('久', 504),\n",
       " ('衰', 501),\n",
       " ('备', 501),\n",
       " ('赏', 499),\n",
       " ('良', 498),\n",
       " ('玉', 498),\n",
       " ('吉', 498),\n",
       " ('虚', 495),\n",
       " ('凶', 494),\n",
       " ('遇', 492),\n",
       " ('葬', 489),\n",
       " ('谏', 489),\n",
       " ('盖', 486),\n",
       " ('疑', 484),\n",
       " ('适', 484),\n",
       " ('舍', 483),\n",
       " ('庶', 483),\n",
       " ('光', 483),\n",
       " ('器', 482),\n",
       " ('军', 482),\n",
       " ('干', 481),\n",
       " ('交', 478),\n",
       " ('兄', 478),\n",
       " ('昭', 477),\n",
       " ('深', 475),\n",
       " ('并', 475),\n",
       " ('厚', 474),\n",
       " ('谋', 472),\n",
       " ('唯', 468),\n",
       " ('海', 467),\n",
       " ('游', 465),\n",
       " ('清', 465),\n",
       " ('汤', 464),\n",
       " ('星', 463),\n",
       " ('辟', 463),\n",
       " ('射', 461),\n",
       " ('灭', 461),\n",
       " ('让', 460),\n",
       " ('积', 460),\n",
       " ('离', 459),\n",
       " ('酒', 458),\n",
       " ('城', 458),\n",
       " ('质', 456),\n",
       " ('慎', 455),\n",
       " ('战', 454),\n",
       " ('征', 453),\n",
       " ('妻', 453),\n",
       " ('丘', 453),\n",
       " ('友', 450),\n",
       " ('面', 450),\n",
       " ('虞', 447),\n",
       " ('降', 446),\n",
       " ('坐', 446),\n",
       " ('予', 445),\n",
       " ('愿', 441),\n",
       " ('首', 441),\n",
       " ('鱼', 440),\n",
       " ('望', 438),\n",
       " ('寒', 436),\n",
       " ('稷', 436),\n",
       " ('惠', 435),\n",
       " ('怨', 435),\n",
       " ('贫', 429),\n",
       " ('鸟', 429),\n",
       " ('轻', 428),\n",
       " ('元', 428),\n",
       " ('财', 425),\n",
       " ('禹', 424),\n",
       " ('隐', 422),\n",
       " ('强', 422),\n",
       " ('载', 421),\n",
       " ('私', 418),\n",
       " ('会', 416),\n",
       " ('灵', 414),\n",
       " ('术', 414),\n",
       " ('堂', 413),\n",
       " ('废', 413),\n",
       " ('音', 413),\n",
       " ('待', 412),\n",
       " ('升', 405),\n",
       " ('越', 405),\n",
       " ('陵', 404),\n",
       " ('争', 403),\n",
       " ('喜', 403),\n",
       " ('号', 403),\n",
       " ('务', 402),\n",
       " ('博', 401),\n",
       " ('报', 400),\n",
       " ('败', 398),\n",
       " ('忘', 397),\n",
       " ('才', 397),\n",
       " ('苟', 396),\n",
       " ('河', 395),\n",
       " ('畏', 395),\n",
       " ('管', 393),\n",
       " ('弃', 393),\n",
       " ('夜', 393),\n",
       " ('罚', 392),\n",
       " ('考', 392),\n",
       " ('属', 391),\n",
       " ('奉', 390),\n",
       " ('宋', 389),\n",
       " ('恐', 389),\n",
       " ('吴', 389),\n",
       " ('鼓', 389),\n",
       " ('攻', 387),\n",
       " ('奸', 386),\n",
       " ('藏', 383),\n",
       " ('张', 382),\n",
       " ('乡', 382),\n",
       " ('等', 382),\n",
       " ('兽', 381),\n",
       " ('颜', 380),\n",
       " ('虎', 379),\n",
       " ('纣', 379),\n",
       " ('共', 378),\n",
       " ('暴', 378),\n",
       " ('景', 377),\n",
       " ('魏', 377),\n",
       " ('更', 375),\n",
       " ('劳', 374),\n",
       " ('改', 373),\n",
       " ('贼', 373),\n",
       " ('禁', 371),\n",
       " ('两', 370),\n",
       " ('遗', 370),\n",
       " ('宾', 368),\n",
       " ('燕', 367),\n",
       " ('怀', 365),\n",
       " ('广', 365),\n",
       " ('愚', 364),\n",
       " ('况', 364),\n",
       " ('卑', 363),\n",
       " ('审', 363),\n",
       " ('宣', 363),\n",
       " ('孰', 362),\n",
       " ('晏', 362),\n",
       " ('除', 359),\n",
       " ('客', 357),\n",
       " ('就', 356),\n",
       " ('草', 356),\n",
       " ('原', 355),\n",
       " ('著', 354),\n",
       " ('渊', 353),\n",
       " ('梁', 353),\n",
       " ('冬', 353),\n",
       " ('献', 352),\n",
       " ('野', 352),\n",
       " ('布', 351),\n",
       " ('庄', 350),\n",
       " ('效', 350),\n",
       " ('代', 349),\n",
       " ('惧', 349),\n",
       " ('牛', 348),\n",
       " ('示', 347),\n",
       " ('男', 347),\n",
       " ('泽', 346),\n",
       " ('设', 345),\n",
       " ('勇', 344),\n",
       " ('俱', 342),\n",
       " ('薄', 341),\n",
       " ('玄', 341),\n",
       " ('初', 340),\n",
       " ('毋', 339),\n",
       " ('州', 339),\n",
       " ('召', 338),\n",
       " ('彊', 338),\n",
       " ('职', 338),\n",
       " ('商', 337),\n",
       " ('击', 337),\n",
       " ('记', 337),\n",
       " ('违', 335),\n",
       " ('仪', 334),\n",
       " ('歌', 333),\n",
       " ('端', 332),\n",
       " ('谨', 331),\n",
       " ('宰', 330),\n",
       " ('寿', 330),\n",
       " ('临', 329),\n",
       " ('期', 329),\n",
       " ('具', 329),\n",
       " ('郊', 329),\n",
       " ('恭', 328),\n",
       " ('辱', 328),\n",
       " ('解', 328),\n",
       " ('韩', 328),\n",
       " ('惑', 327),\n",
       " ('息', 324),\n",
       " ('序', 323),\n",
       " ('业', 323),\n",
       " ('申', 322),\n",
       " ('手', 322),\n",
       " ('验', 321),\n",
       " ('肉', 319),\n",
       " ('纪', 319),\n",
       " ('幸', 318),\n",
       " ('尺', 318),\n",
       " ('列', 318),\n",
       " ('灾', 317),\n",
       " ('还', 314),\n",
       " ('计', 312),\n",
       " ('勿', 311),\n",
       " ('持', 311),\n",
       " ('犯', 309),\n",
       " ('简', 309),\n",
       " ('似', 309),\n",
       " ('免', 307),\n",
       " ('尹', 307),\n",
       " ('权', 307),\n",
       " ('率', 307),\n",
       " ('淫', 306),\n",
       " ('逆', 306),\n",
       " ('承', 304),\n",
       " ('工', 302),\n",
       " ('推', 301),\n",
       " ('县', 301),\n",
       " ('救', 300),\n",
       " ('兼', 300),\n",
       " ('克', 300),\n",
       " ('甲', 300),\n",
       " ('农', 299),\n",
       " ('随', 299),\n",
       " ('显', 299),\n",
       " ('再', 298),\n",
       " ('避', 296),\n",
       " ('兮', 295),\n",
       " ('鸣', 295),\n",
       " ('弱', 295),\n",
       " ('伏', 294),\n",
       " ('凤', 293),\n",
       " ('桀', 293),\n",
       " ('汝', 293),\n",
       " ('邑', 291),\n",
       " ('盗', 290),\n",
       " ('虑', 288),\n",
       " ('几', 285),\n",
       " ('温', 284),\n",
       " ('寝', 284),\n",
       " ('毁', 283),\n",
       " ('戒', 283),\n",
       " ('钟', 283),\n",
       " ('廉', 283),\n",
       " ('继', 281),\n",
       " ('断', 281),\n",
       " ('末', 280),\n",
       " ('华', 276),\n",
       " ('顾', 276),\n",
       " ('议', 275),\n",
       " ('雷', 275),\n",
       " ('川', 274),\n",
       " ('宁', 273),\n",
       " ('获', 273),\n",
       " ('饰', 273),\n",
       " ('恩', 273),\n",
       " ('厉', 270),\n",
       " ('被', 270),\n",
       " ('奚', 269),\n",
       " ('空', 269),\n",
       " ('折', 269),\n",
       " ('霸', 269),\n",
       " ('祝', 268),\n",
       " ('户', 268),\n",
       " ('习', 267),\n",
       " ('奇', 267),\n",
       " ('参', 266),\n",
       " ('耻', 265),\n",
       " ('材', 264),\n",
       " ('毕', 264),\n",
       " ('幽', 264),\n",
       " ('操', 264),\n",
       " ('虫', 264),\n",
       " ('造', 263),\n",
       " ('仕', 261),\n",
       " ('台', 260),\n",
       " ('侍', 259),\n",
       " ('幼', 259),\n",
       " ('妄', 259),\n",
       " ('负', 258),\n",
       " ('感', 258),\n",
       " ('须', 257),\n",
       " ('辩', 257),\n",
       " ('要', 256),\n",
       " ('荣', 256),\n",
       " ('肖', 256),\n",
       " ('识', 255),\n",
       " ('历', 255),\n",
       " ('祥', 255),\n",
       " ('遭', 255),\n",
       " ('卜', 254),\n",
       " ('他', 253),\n",
       " ('择', 252),\n",
       " ('贞', 252),\n",
       " ('字', 252),\n",
       " ('羊', 251),\n",
       " ('否', 251),\n",
       " ('怪', 250),\n",
       " ('辨', 250),\n",
       " ('族', 250),\n",
       " ('建', 250),\n",
       " ('禽', 249),\n",
       " ('赤', 249),\n",
       " ('曲', 248),\n",
       " ('尸', 248),\n",
       " ('附', 248),\n",
       " ('江', 248),\n",
       " ('树', 247),\n",
       " ('夺', 247),\n",
       " ('假', 247),\n",
       " ('羽', 246),\n",
       " ('血', 245),\n",
       " ('墨', 245),\n",
       " ('济', 243),\n",
       " ('置', 243),\n",
       " ('授', 242),\n",
       " ('辅', 241),\n",
       " ('咸', 241),\n",
       " ('表', 240),\n",
       " ('郑', 240),\n",
       " ('回', 239),\n",
       " ('损', 239),\n",
       " ('短', 239),\n",
       " ('专', 239),\n",
       " ('势', 239),\n",
       " ('屈', 239),\n",
       " ('梦', 238),\n",
       " ('席', 238),\n",
       " ('典', 238),\n",
       " ('增', 238),\n",
       " ('素', 237),\n",
       " ('谁', 237),\n",
       " ('采', 237),\n",
       " ('稽', 236),\n",
       " ('趋', 235),\n",
       " ('饥', 235),\n",
       " ('穆', 234),\n",
       " ('旧', 234),\n",
       " ('覆', 234),\n",
       " ('引', 234),\n",
       " ('塞', 233),\n",
       " ('急', 233),\n",
       " ('郡', 233),\n",
       " ('省', 232),\n",
       " ('履', 232),\n",
       " ('迎', 232),\n",
       " ('杖', 231),\n",
       " ('移', 231),\n",
       " ('敌', 231),\n",
       " ('陛', 231),\n",
       " ('图', 230),\n",
       " ('吊', 230),\n",
       " ('敛', 229),\n",
       " ('休', 229),\n",
       " ('都', 229),\n",
       " ('案', 229),\n",
       " ('耕', 227),\n",
       " ('党', 226),\n",
       " ('贪', 226),\n",
       " ('崩', 225),\n",
       " ('保', 224),\n",
       " ('市', 224),\n",
       " ('寸', 224),\n",
       " ('篇', 222),\n",
       " ('唐', 221),\n",
       " ('苦', 221),\n",
       " ('迁', 219),\n",
       " ('走', 219),\n",
       " ('困', 218),\n",
       " ('责', 218),\n",
       " ('角', 217),\n",
       " ('庸', 217),\n",
       " ('甘', 217),\n",
       " ('悦', 217),\n",
       " ('新', 216),\n",
       " ('弓', 216),\n",
       " ('徙', 216),\n",
       " ('向', 216),\n",
       " ('满', 216),\n",
       " ('诈', 215),\n",
       " ('牲', 215),\n",
       " ('累', 213),\n",
       " ('厥', 212),\n",
       " ('伪', 211),\n",
       " ('送', 210),\n",
       " ('狱', 210),\n",
       " ('骨', 210),\n",
       " ('指', 209),\n",
       " ('产', 209),\n",
       " ('尼', 209),\n",
       " ('策', 208),\n",
       " ('府', 207),\n",
       " ('约', 206),\n",
       " ('味', 206),\n",
       " ('骄', 205),\n",
       " ('殆', 205),\n",
       " ('戚', 205),\n",
       " ('蒙', 205),\n",
       " ('接', 205),\n",
       " ('助', 204),\n",
       " ('宝', 203),\n",
       " ('胥', 203),\n",
       " ('巧', 202),\n",
       " ('胡', 202),\n",
       " ('统', 202),\n",
       " ('襄', 201),\n",
       " ('屋', 201),\n",
       " ('誉', 200),\n",
       " ('蔽', 199),\n",
       " ('逾', 199),\n",
       " ('孤', 199),\n",
       " ('决', 199),\n",
       " ('律', 199),\n",
       " ('收', 199),\n",
       " ('许', 198),\n",
       " ('留', 198),\n",
       " ('傅', 198),\n",
       " ('殊', 198),\n",
       " ('坚', 197),\n",
       " ('某', 197),\n",
       " ('便', 196),\n",
       " ('崇', 196),\n",
       " ('选', 196),\n",
       " ('鼎', 196),\n",
       " ('阶', 195),\n",
       " ('嘉', 195),\n",
       " ('散', 195),\n",
       " ('妖', 195),\n",
       " ('放', 194),\n",
       " ('带', 194),\n",
       " ('据', 194),\n",
       " ('疏', 193),\n",
       " ('朱', 193),\n",
       " ('隆', 193),\n",
       " ('调', 193),\n",
       " ('开', 192),\n",
       " ('聚', 192),\n",
       " ('冥', 192),\n",
       " ('垂', 191),\n",
       " ('痛', 191),\n",
       " ('闲', 190),\n",
       " ('扬', 190),\n",
       " ('狄', 189),\n",
       " ('窃', 189),\n",
       " ('集', 189),\n",
       " ('登', 189),\n",
       " ('介', 189),\n",
       " ('真', 189),\n",
       " ('林', 188),\n",
       " ('貌', 188),\n",
       " ('果', 187),\n",
       " ('畜', 187),\n",
       " ('齿', 187),\n",
       " ('龟', 187),\n",
       " ('瑞', 187),\n",
       " ('佞', 186),\n",
       " ('货', 186),\n",
       " ('全', 186),\n",
       " ('静', 185),\n",
       " ('矢', 185),\n",
       " ('悲', 185),\n",
       " ('宅', 185),\n",
       " ('丑', 185),\n",
       " ('量', 184),\n",
       " ('严', 184),\n",
       " ('状', 184),\n",
       " ('关', 182),\n",
       " ('延', 182),\n",
       " ('桑', 181),\n",
       " ('兆', 181),\n",
       " ('讳', 181),\n",
       " ('呼', 180),\n",
       " ('贾', 180),\n",
       " ('迹', 180),\n",
       " ('旱', 180),\n",
       " ('差', 180),\n",
       " ('柔', 179),\n",
       " ('奏', 179),\n",
       " ('康', 178),\n",
       " ('琴', 178),\n",
       " ('暗', 178),\n",
       " ('诏', 178),\n",
       " ('劝', 177),\n",
       " ('廷', 177),\n",
       " ('仰', 176),\n",
       " ('连', 176),\n",
       " ('破', 176),\n",
       " ('占', 175),\n",
       " ('舒', 175),\n",
       " ('忍', 174),\n",
       " ('妾', 174),\n",
       " ('黑', 174),\n",
       " ('宽', 173),\n",
       " ('纳', 173),\n",
       " ('庭', 172),\n",
       " ('愈', 172),\n",
       " ('厌', 172),\n",
       " ('皮', 171),\n",
       " ('倍', 171),\n",
       " ('袭', 171),\n",
       " ('勤', 170),\n",
       " ('刺', 170),\n",
       " ('偶', 170),\n",
       " ('靡', 169),\n",
       " ('慈', 168),\n",
       " ('昆', 168),\n",
       " ('陶', 168),\n",
       " ('伊', 168),\n",
       " ('姑', 168),\n",
       " ('飞', 168),\n",
       " ('李', 168),\n",
       " ('毛', 168),\n",
       " ('墓', 168),\n",
       " ('笑', 167),\n",
       " ('刚', 167),\n",
       " ('腹', 167),\n",
       " ('泣', 166),\n",
       " ('寇', 166),\n",
       " ('咎', 165),\n",
       " ('雅', 165),\n",
       " ('衡', 165),\n",
       " ('索', 165),\n",
       " ('朋', 164),\n",
       " ('没', 164),\n",
       " ('朔', 164),\n",
       " ('叹', 164),\n",
       " ('逐', 164),\n",
       " ('邦', 163),\n",
       " ('粟', 163),\n",
       " ('荐', 163),\n",
       " ('泉', 163),\n",
       " ('委', 163),\n",
       " ('追', 162),\n",
       " ('侧', 162),\n",
       " ('仆', 162),\n",
       " ('伦', 162),\n",
       " ('优', 161),\n",
       " ('佐', 161),\n",
       " ('婴', 160),\n",
       " ('井', 159),\n",
       " ('戎', 159),\n",
       " ('仓', 159),\n",
       " ('革', 159),\n",
       " ('尉', 159),\n",
       " ('嗣', 158),\n",
       " ('舞', 157),\n",
       " ('盈', 157),\n",
       " ('翼', 157),\n",
       " ('荆', 157),\n",
       " ('斗', 157),\n",
       " ('鸡', 157),\n",
       " ('吕', 157),\n",
       " ('邻', 156),\n",
       " ('舆', 155),\n",
       " ('涂', 155),\n",
       " ('剑', 155),\n",
       " ('庆', 155),\n",
       " ('结', 155),\n",
       " ('欺', 154),\n",
       " ('豫', 154),\n",
       " ('肯', 154),\n",
       " ('错', 153),\n",
       " ('境', 153),\n",
       " ('赦', 152),\n",
       " ('舟', 152),\n",
       " ('均', 152),\n",
       " ('种', 152),\n",
       " ('泰', 151),\n",
       " ('洁', 151),\n",
       " ('露', 151),\n",
       " ('触', 151),\n",
       " ('譬', 150),\n",
       " ('雍', 150),\n",
       " ('聪', 150),\n",
       " ('头', 150),\n",
       " ('丰', 150),\n",
       " ('忌', 150),\n",
       " ('禀', 150),\n",
       " ('纯', 149),\n",
       " ('享', 149),\n",
       " ('帛', 149),\n",
       " ('步', 149),\n",
       " ('郭', 149),\n",
       " ('雠', 149),\n",
       " ('辰', 148),\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['你', '好', '，', '世', '界', '！', '这', '是', '一', '个', '示', '例', '文', '本', '。']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('SIKU-BERT/sikubert')\n",
    "\n",
    "# 示例文本\n",
    "text = \"你好，世界！这是一个示例文本。\"\n",
    "\n",
    "# 使用分词器\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wayer/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-16 10:26:16,546 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 394/394 [03:48<00:00,  1.72it/s]\n",
      "2024-04-16 10:30:08,782 - BERTopic - Embedding - Completed ✓\n",
      "2024-04-16 10:30:08,783 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-04-16 10:30:26,289 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-04-16 10:30:26,290 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-04-16 10:30:30,053 - BERTopic - Cluster - Completed ✓\n",
      "2024-04-16 10:30:30,060 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-04-16 10:30:30,808 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(language=\"multilingual\", calculate_probabilities=True, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(data['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_model.get_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>6634</td>\n",
       "      <td>-1_孔子曰_子曰_春秋_故曰</td>\n",
       "      <td>[孔子曰, 子曰, 春秋, 故曰, 或曰, 何也, 此之谓也, 对曰, 公曰, 尚书]</td>\n",
       "      <td>[穆公问于子思曰：“立太子有常乎？”荅曰：“有之。在周公之典。”公曰：“昔文王舍适而立其次，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>868</td>\n",
       "      <td>0_对曰_王曰_公曰_孔子曰</td>\n",
       "      <td>[对曰, 王曰, 公曰, 孔子曰, 简子曰, 何也, 文侯曰, 桓公曰, 春秋, 管仲曰]</td>\n",
       "      <td>[他日，简子出，有人当道，辟之不去，从者将刃，当道者曰：“吾有欲谒于主君。”从者以闻。简子召...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>697</td>\n",
       "      <td>1____</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[, , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>434</td>\n",
       "      <td>2_测曰_次八_次六_次七</td>\n",
       "      <td>[测曰, 次八, 次六, 次七, 上九, 次五, 还于丧, 崔嵬不崩, 角解豸, 奔鹿怀鼷]</td>\n",
       "      <td>[次七，竦萃于丘冢。测曰，“竦萃丘冢”、礼不废也。, 次八，大敛大巅。测曰，“大敛”之“巅”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>268</td>\n",
       "      <td>3_孟子曰_孟子对曰_人皆有之_吾闻西伯善养老者</td>\n",
       "      <td>[孟子曰, 孟子对曰, 人皆有之, 吾闻西伯善养老者, 予何为不受, 盍归乎来, 未之有也,...</td>\n",
       "      <td>[孟子曰：“何以言之？”, 孟子曰：“然。”, 孟子曰：“礼也。”]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>203</td>\n",
       "      <td>4_经礼也_有世妇_有妾_有妻</td>\n",
       "      <td>[经礼也, 有世妇, 有妾, 有妻, 春秋传, 大夫曰孺人, 士曰妇人, 昏礼经, 为之赐,...</td>\n",
       "      <td>[《春秋》有经礼，有变礼。为如安性平心者，经礼也。至有于性，虽不安，于心，虽不平，于道，无以...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>182</td>\n",
       "      <td>5_孔子对曰_孔子曰_公曰_哀公曰</td>\n",
       "      <td>[孔子对曰, 孔子曰, 公曰, 哀公曰, 哀公问于孔子曰, 子路曰, 服使然也, 而君不悟,...</td>\n",
       "      <td>[哀公问于孔子曰：“大礼何如？子之言礼，何其尊？”孔子对曰：“丘也鄙人，不足以知大礼。”公曰...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>159</td>\n",
       "      <td>6_次二_初一_测曰_冲冲儿遇</td>\n",
       "      <td>[次二, 初一, 测曰, 冲冲儿遇, 孚其肉, 赫河, 争不争, 将造邪, 家无壶, 谨于媐㐜]</td>\n",
       "      <td>[初一，丸钻钻于内隙，厉。测曰，“丸钻于内”、转丸非也。, 次二，𫔵无间。测曰，“无间”之“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>141</td>\n",
       "      <td>7_春秋_则不得其正_其言之成理_足以欺惑愚众</td>\n",
       "      <td>[春秋, 则不得其正, 其言之成理, 足以欺惑愚众, 然而其持之有故, 无之有也, 如此者,...</td>\n",
       "      <td>[彦曰：“人之性分气度不同，有体貌亢䟽，色厉矜庄，仪容冰栗，似若能断，而当事少决，不遂其为者...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>127</td>\n",
       "      <td>8_如雨_尔雅_星𫕥如雨_何以验之</td>\n",
       "      <td>[如雨, 尔雅, 星𫕥如雨, 何以验之, 春秋, 昏而中, 夜中, 日月之行, 下布之民, 不修]</td>\n",
       "      <td>[礼者，继天地，体阴阳，而慎主客，序尊卑、贵贱、大小之位，而差外内、远近、新故之级者也，以德...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                      Name  \\\n",
       "0     -1   6634           -1_孔子曰_子曰_春秋_故曰   \n",
       "1      0    868            0_对曰_王曰_公曰_孔子曰   \n",
       "2      1    697                     1____   \n",
       "3      2    434             2_测曰_次八_次六_次七   \n",
       "4      3    268  3_孟子曰_孟子对曰_人皆有之_吾闻西伯善养老者   \n",
       "5      4    203           4_经礼也_有世妇_有妾_有妻   \n",
       "6      5    182         5_孔子对曰_孔子曰_公曰_哀公曰   \n",
       "7      6    159           6_次二_初一_测曰_冲冲儿遇   \n",
       "8      7    141   7_春秋_则不得其正_其言之成理_足以欺惑愚众   \n",
       "9      8    127         8_如雨_尔雅_星𫕥如雨_何以验之   \n",
       "\n",
       "                                      Representation  \\\n",
       "0        [孔子曰, 子曰, 春秋, 故曰, 或曰, 何也, 此之谓也, 对曰, 公曰, 尚书]   \n",
       "1      [对曰, 王曰, 公曰, 孔子曰, 简子曰, 何也, 文侯曰, 桓公曰, 春秋, 管仲曰]   \n",
       "2                               [, , , , , , , , , ]   \n",
       "3     [测曰, 次八, 次六, 次七, 上九, 次五, 还于丧, 崔嵬不崩, 角解豸, 奔鹿怀鼷]   \n",
       "4  [孟子曰, 孟子对曰, 人皆有之, 吾闻西伯善养老者, 予何为不受, 盍归乎来, 未之有也,...   \n",
       "5  [经礼也, 有世妇, 有妾, 有妻, 春秋传, 大夫曰孺人, 士曰妇人, 昏礼经, 为之赐,...   \n",
       "6  [孔子对曰, 孔子曰, 公曰, 哀公曰, 哀公问于孔子曰, 子路曰, 服使然也, 而君不悟,...   \n",
       "7   [次二, 初一, 测曰, 冲冲儿遇, 孚其肉, 赫河, 争不争, 将造邪, 家无壶, 谨于媐㐜]   \n",
       "8  [春秋, 则不得其正, 其言之成理, 足以欺惑愚众, 然而其持之有故, 无之有也, 如此者,...   \n",
       "9  [如雨, 尔雅, 星𫕥如雨, 何以验之, 春秋, 昏而中, 夜中, 日月之行, 下布之民, 不修]   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [穆公问于子思曰：“立太子有常乎？”荅曰：“有之。在周公之典。”公曰：“昔文王舍适而立其次，...  \n",
       "1  [他日，简子出，有人当道，辟之不去，从者将刃，当道者曰：“吾有欲谒于主君。”从者以闻。简子召...  \n",
       "2                                             [, , ]  \n",
       "3  [次七，竦萃于丘冢。测曰，“竦萃丘冢”、礼不废也。, 次八，大敛大巅。测曰，“大敛”之“巅”...  \n",
       "4                 [孟子曰：“何以言之？”, 孟子曰：“然。”, 孟子曰：“礼也。”]  \n",
       "5  [《春秋》有经礼，有变礼。为如安性平心者，经礼也。至有于性，虽不安，于心，虽不平，于道，无以...  \n",
       "6  [哀公问于孔子曰：“大礼何如？子之言礼，何其尊？”孔子对曰：“丘也鄙人，不足以知大礼。”公曰...  \n",
       "7  [初一，丸钻钻于内隙，厉。测曰，“丸钻于内”、转丸非也。, 次二，𫔵无间。测曰，“无间”之“...  \n",
       "8  [彦曰：“人之性分气度不同，有体貌亢䟽，色厉矜庄，仪容冰栗，似若能断，而当事少决，不遂其为者...  \n",
       "9  [礼者，继天地，体阴阳，而慎主客，序尊卑、贵贱、大小之位，而差外内、远近、新故之级者也，以德...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 10:32:49,157 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-04-16 10:32:49,903 - BERTopic - Topic reduction - Reduced number of topics from 95 to 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x7fa54a744730>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.reduce_topics(data['sentences'], nr_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>6799</td>\n",
       "      <td>-1_孔子曰_子曰_春秋_对曰</td>\n",
       "      <td>[孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]</td>\n",
       "      <td>[樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4521</td>\n",
       "      <td>0_测曰_孔子曰_孟子曰_子曰</td>\n",
       "      <td>[测曰, 孔子曰, 孟子曰, 子曰, 春秋, 对曰, 公曰, 或问, 何也, 次八]</td>\n",
       "      <td>[上九，不终其德，二岁见代。测曰，“不终”之“代”、不可长也。, 次七，乘火寒泉至。测曰，“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>937</td>\n",
       "      <td>1_散也_字子从_戟也_善也</td>\n",
       "      <td>[散也, 字子从, 戟也, 善也, 升也, 取也, 数也, 长也, 燕伋, 狄黑]</td>\n",
       "      <td>[取荼。荼也者，以为君荐蒋也。, 跬，一举足也。倍跬，谓之歩。, 薛邦，字子从。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>2_长三尺_非也_左画苍龙_两臂前后刻金</td>\n",
       "      <td>[长三尺, 非也, 左画苍龙, 两臂前后刻金, 金鍐形如缇, 系轴头, 金燧, 金鍐方𨰿, ...</td>\n",
       "      <td>[谷之始熟曰粟，舂之于臼，簸其秕糠，蒸之于甑，爨之以火，成熟为饭，乃甘可食。可食而食之，味生...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>3_行夏令_行春令_行秋令_行冬令</td>\n",
       "      <td>[行夏令, 行春令, 行秋令, 行冬令, 则其国乃旱, 四鄙入保, 风从虎, 云从龙, 则其...</td>\n",
       "      <td>[然夫虫之生也，必依温湿。温湿之气常在春夏，秋冬之气寒而干燥，虫未曾生。若以虫生，罪乡部吏，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>4_鲁人_少孔子五十岁_名山大泽不以封_其余</td>\n",
       "      <td>[鲁人, 少孔子五十岁, 名山大泽不以封, 其余, 三月不从政, 其余以为附庸间田, 一坐再...</td>\n",
       "      <td>[方千里者，为方百里者百。封方百里者三十国，其余，方百里者七十。又封方七十里者六十，为方百里...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>5_太尉沛国刘矩_弘农太守河内吴匡_韩安国_城阳景王祠</td>\n",
       "      <td>[太尉沛国刘矩, 弘农太守河内吴匡, 韩安国, 城阳景王祠, 九江太守武陵陈子威, 公车征士...</td>\n",
       "      <td>[*太尉沛国刘矩, 鼓：○□○○□□○□○○□。半：○□○□○○○□□○□○。鲁鼓。○□○○...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>6_九月_王狩_十月_十二月也</td>\n",
       "      <td>[九月, 王狩, 十月, 十二月也, 十一月, 四月, 六月, 七月, 二月, 三月]</td>\n",
       "      <td>[九月：, 或说《春秋》、十二月也。, 十一月：王狩。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7_衣青衣_载白旗_服仓玉_驾仓龙</td>\n",
       "      <td>[衣青衣, 载白旗, 服仓玉, 驾仓龙, 服白玉, 服赤玉, 衣朱衣, 乘戎路, 食麻与犬,...</td>\n",
       "      <td>[天子居青阳大庙，乘鸾路，驾仓龙，载青旗，衣青衣，服仓玉，食麦与羊，其器疏以达。, 天子居明...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>8_此虚言也_殆虚言也_如实论之_则政不核</td>\n",
       "      <td>[此虚言也, 殆虚言也, 如实论之, 则政不核, 载以为是, 如真不真, 原省其实, 此虚言...</td>\n",
       "      <td>[曰：此虚言也。, 曰：此虚言也。, 此虚言也。]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                         Name  \\\n",
       "0     -1   6799              -1_孔子曰_子曰_春秋_对曰   \n",
       "1      0   4521              0_测曰_孔子曰_孟子曰_子曰   \n",
       "2      1    937               1_散也_字子从_戟也_善也   \n",
       "3      2    146         2_长三尺_非也_左画苍龙_两臂前后刻金   \n",
       "4      3     80            3_行夏令_行春令_行秋令_行冬令   \n",
       "5      4     50       4_鲁人_少孔子五十岁_名山大泽不以封_其余   \n",
       "6      5     36  5_太尉沛国刘矩_弘农太守河内吴匡_韩安国_城阳景王祠   \n",
       "7      6     13              6_九月_王狩_十月_十二月也   \n",
       "8      7     11            7_衣青衣_载白旗_服仓玉_驾仓龙   \n",
       "9      8     11        8_此虚言也_殆虚言也_如实论之_则政不核   \n",
       "\n",
       "                                      Representation  \\\n",
       "0        [孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]   \n",
       "1         [测曰, 孔子曰, 孟子曰, 子曰, 春秋, 对曰, 公曰, 或问, 何也, 次八]   \n",
       "2          [散也, 字子从, 戟也, 善也, 升也, 取也, 数也, 长也, 燕伋, 狄黑]   \n",
       "3  [长三尺, 非也, 左画苍龙, 两臂前后刻金, 金鍐形如缇, 系轴头, 金燧, 金鍐方𨰿, ...   \n",
       "4  [行夏令, 行春令, 行秋令, 行冬令, 则其国乃旱, 四鄙入保, 风从虎, 云从龙, 则其...   \n",
       "5  [鲁人, 少孔子五十岁, 名山大泽不以封, 其余, 三月不从政, 其余以为附庸间田, 一坐再...   \n",
       "6  [太尉沛国刘矩, 弘农太守河内吴匡, 韩安国, 城阳景王祠, 九江太守武陵陈子威, 公车征士...   \n",
       "7        [九月, 王狩, 十月, 十二月也, 十一月, 四月, 六月, 七月, 二月, 三月]   \n",
       "8  [衣青衣, 载白旗, 服仓玉, 驾仓龙, 服白玉, 服赤玉, 衣朱衣, 乘戎路, 食麻与犬,...   \n",
       "9  [此虚言也, 殆虚言也, 如实论之, 则政不核, 载以为是, 如真不真, 原省其实, 此虚言...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...  \n",
       "1  [上九，不终其德，二岁见代。测曰，“不终”之“代”、不可长也。, 次七，乘火寒泉至。测曰，“...  \n",
       "2          [取荼。荼也者，以为君荐蒋也。, 跬，一举足也。倍跬，谓之歩。, 薛邦，字子从。]  \n",
       "3  [谷之始熟曰粟，舂之于臼，簸其秕糠，蒸之于甑，爨之以火，成熟为饭，乃甘可食。可食而食之，味生...  \n",
       "4  [然夫虫之生也，必依温湿。温湿之气常在春夏，秋冬之气寒而干燥，虫未曾生。若以虫生，罪乡部吏，...  \n",
       "5  [方千里者，为方百里者百。封方百里者三十国，其余，方百里者七十。又封方七十里者六十，为方百里...  \n",
       "6  [*太尉沛国刘矩, 鼓：○□○○□□○□○○□。半：○□○□○○○□□○□○。鲁鼓。○□○○...  \n",
       "7                       [九月：, 或说《春秋》、十二月也。, 十一月：王狩。]  \n",
       "8  [天子居青阳大庙，乘鸾路，驾仓龙，载青旗，衣青衣，服仓玉，食麦与羊，其器疏以达。, 天子居明...  \n",
       "9                          [曰：此虚言也。, 曰：此虚言也。, 此虚言也。]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('测曰', 0.02865606933188256),\n",
       " ('孔子曰', 0.014110274406217654),\n",
       " ('孟子曰', 0.012987624795505987),\n",
       " ('子曰', 0.01121131113435812),\n",
       " ('春秋', 0.007015365708228161),\n",
       " ('对曰', 0.006602798744358283),\n",
       " ('公曰', 0.006395234385509569),\n",
       " ('或问', 0.005959937313216265),\n",
       " ('何也', 0.005644846168066878),\n",
       " ('次八', 0.005518285563469141)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？”</td>\n",
       "      <td>0</td>\n",
       "      <td>0_测曰_孔子曰_孟子曰_子曰</td>\n",
       "      <td>[测曰, 孔子曰, 孟子曰, 子曰, 春秋, 对曰, 公曰, 或问, 何也, 次八]</td>\n",
       "      <td>[上九，不终其德，二岁见代。测曰，“不终”之“代”、不可长也。, 次七，乘火寒泉至。测曰，“...</td>\n",
       "      <td>测曰 - 孔子曰 - 孟子曰 - 子曰 - 春秋 - 对曰 - 公曰 - 或问 - 何也 - 次八</td>\n",
       "      <td>0.641458</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>有子曰：“其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_孔子曰_子曰_春秋_对曰</td>\n",
       "      <td>[孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]</td>\n",
       "      <td>[樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...</td>\n",
       "      <td>孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...</td>\n",
       "      <td>0.197736</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>子曰：“巧言令色，鲜矣仁！”</td>\n",
       "      <td>0</td>\n",
       "      <td>0_测曰_孔子曰_孟子曰_子曰</td>\n",
       "      <td>[测曰, 孔子曰, 孟子曰, 子曰, 春秋, 对曰, 公曰, 或问, 何也, 次八]</td>\n",
       "      <td>[上九，不终其德，二岁见代。测曰，“不终”之“代”、不可长也。, 次七，乘火寒泉至。测曰，“...</td>\n",
       "      <td>测曰 - 孔子曰 - 孟子曰 - 子曰 - 春秋 - 对曰 - 公曰 - 或问 - 何也 - 次八</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>曾子曰：“吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？”</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_孔子曰_子曰_春秋_对曰</td>\n",
       "      <td>[孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]</td>\n",
       "      <td>[樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...</td>\n",
       "      <td>孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...</td>\n",
       "      <td>0.420131</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>子曰：“道千乘之国：敬事而信，节用而爱人，使民以时。”</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_孔子曰_子曰_春秋_对曰</td>\n",
       "      <td>[孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]</td>\n",
       "      <td>[樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...</td>\n",
       "      <td>孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...</td>\n",
       "      <td>0.134986</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>樊哙冠，汉将军樊哙造次所冠，以入项籍营，广七寸，前出四寸，司马殿门大护卫士服之。</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_孔子曰_子曰_春秋_对曰</td>\n",
       "      <td>[孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]</td>\n",
       "      <td>[樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...</td>\n",
       "      <td>孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...</td>\n",
       "      <td>0.892590</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12600</th>\n",
       "      <td>却敌冠、前高四寸，通长四寸，后高三寸，监门卫士服之，《礼》无文。</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_孔子曰_子曰_春秋_对曰</td>\n",
       "      <td>[孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]</td>\n",
       "      <td>[樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...</td>\n",
       "      <td>孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...</td>\n",
       "      <td>0.893724</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12601</th>\n",
       "      <td>珠冕、爵冔收、通天冠、进贤冠、长冠、缁布冠、委貌冠、皮弁、惠文冠，古者天子冠所加者，其次在汉礼。</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_孔子曰_子曰_春秋_对曰</td>\n",
       "      <td>[孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]</td>\n",
       "      <td>[樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...</td>\n",
       "      <td>孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...</td>\n",
       "      <td>0.091955</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12602</th>\n",
       "      <td>帝谥：违拂不成曰隐，靖民则法曰黄，翼善传圣曰尧，仁圣盛明曰舜，残人多垒曰桀，残义损善曰纣，慈...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_测曰_孔子曰_孟子曰_子曰</td>\n",
       "      <td>[测曰, 孔子曰, 孟子曰, 子曰, 春秋, 对曰, 公曰, 或问, 何也, 次八]</td>\n",
       "      <td>[上九，不终其德，二岁见代。测曰，“不终”之“代”、不可长也。, 次七，乘火寒泉至。测曰，“...</td>\n",
       "      <td>测曰 - 孔子曰 - 孟子曰 - 子曰 - 春秋 - 对曰 - 公曰 - 或问 - 何也 - 次八</td>\n",
       "      <td>0.929501</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12603</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1_散也_字子从_戟也_善也</td>\n",
       "      <td>[散也, 字子从, 戟也, 善也, 升也, 取也, 数也, 长也, 燕伋, 狄黑]</td>\n",
       "      <td>[取荼。荼也者，以为君荐蒋也。, 跬，一举足也。倍跬，谓之歩。, 薛邦，字子从。]</td>\n",
       "      <td>散也 - 字子从 - 戟也 - 善也 - 升也 - 取也 - 数也 - 长也 - 燕伋 - 狄黑</td>\n",
       "      <td>0.574019</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12604 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Document  Topic  \\\n",
       "0              子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？”      0   \n",
       "1      有子曰：“其为人也孝弟，而好犯上者，鲜矣；不好犯上，而好作乱者，未之有也。君子务本，本立而道...     -1   \n",
       "2                                         子曰：“巧言令色，鲜矣仁！”      0   \n",
       "3                    曾子曰：“吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？”     -1   \n",
       "4                            子曰：“道千乘之国：敬事而信，节用而爱人，使民以时。”     -1   \n",
       "...                                                  ...    ...   \n",
       "12599           樊哙冠，汉将军樊哙造次所冠，以入项籍营，广七寸，前出四寸，司马殿门大护卫士服之。     -1   \n",
       "12600                   却敌冠、前高四寸，通长四寸，后高三寸，监门卫士服之，《礼》无文。     -1   \n",
       "12601   珠冕、爵冔收、通天冠、进贤冠、长冠、缁布冠、委貌冠、皮弁、惠文冠，古者天子冠所加者，其次在汉礼。     -1   \n",
       "12602  帝谥：违拂不成曰隐，靖民则法曰黄，翼善传圣曰尧，仁圣盛明曰舜，残人多垒曰桀，残义损善曰纣，慈...      0   \n",
       "12603                                                         1   \n",
       "\n",
       "                  Name                               Representation  \\\n",
       "0      0_测曰_孔子曰_孟子曰_子曰   [测曰, 孔子曰, 孟子曰, 子曰, 春秋, 对曰, 公曰, 或问, 何也, 次八]   \n",
       "1      -1_孔子曰_子曰_春秋_对曰  [孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]   \n",
       "2      0_测曰_孔子曰_孟子曰_子曰   [测曰, 孔子曰, 孟子曰, 子曰, 春秋, 对曰, 公曰, 或问, 何也, 次八]   \n",
       "3      -1_孔子曰_子曰_春秋_对曰  [孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]   \n",
       "4      -1_孔子曰_子曰_春秋_对曰  [孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]   \n",
       "...                ...                                          ...   \n",
       "12599  -1_孔子曰_子曰_春秋_对曰  [孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]   \n",
       "12600  -1_孔子曰_子曰_春秋_对曰  [孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]   \n",
       "12601  -1_孔子曰_子曰_春秋_对曰  [孔子曰, 子曰, 春秋, 对曰, 故曰, 何也, 或曰, 公曰, 此之谓也, 尚书]   \n",
       "12602  0_测曰_孔子曰_孟子曰_子曰   [测曰, 孔子曰, 孟子曰, 子曰, 春秋, 对曰, 公曰, 或问, 何也, 次八]   \n",
       "12603   1_散也_字子从_戟也_善也    [散也, 字子从, 戟也, 善也, 升也, 取也, 数也, 长也, 燕伋, 狄黑]   \n",
       "\n",
       "                                     Representative_Docs  \\\n",
       "0      [上九，不终其德，二岁见代。测曰，“不终”之“代”、不可长也。, 次七，乘火寒泉至。测曰，“...   \n",
       "1      [樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...   \n",
       "2      [上九，不终其德，二岁见代。测曰，“不终”之“代”、不可长也。, 次七，乘火寒泉至。测曰，“...   \n",
       "3      [樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...   \n",
       "4      [樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...   \n",
       "...                                                  ...   \n",
       "12599  [樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...   \n",
       "12600  [樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...   \n",
       "12601  [樊迟问仁。子曰：“爱人。”问知。子曰：“知人。”樊迟未达。子曰：“举直错诸枉，能使枉者直。...   \n",
       "12602  [上九，不终其德，二岁见代。测曰，“不终”之“代”、不可长也。, 次七，乘火寒泉至。测曰，“...   \n",
       "12603          [取荼。荼也者，以为君荐蒋也。, 跬，一举足也。倍跬，谓之歩。, 薛邦，字子从。]   \n",
       "\n",
       "                                             Top_n_words  Probability  \\\n",
       "0      测曰 - 孔子曰 - 孟子曰 - 子曰 - 春秋 - 对曰 - 公曰 - 或问 - 何也 - 次八     0.641458   \n",
       "1      孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...     0.197736   \n",
       "2      测曰 - 孔子曰 - 孟子曰 - 子曰 - 春秋 - 对曰 - 公曰 - 或问 - 何也 - 次八     1.000000   \n",
       "3      孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...     0.420131   \n",
       "4      孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...     0.134986   \n",
       "...                                                  ...          ...   \n",
       "12599  孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...     0.892590   \n",
       "12600  孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...     0.893724   \n",
       "12601  孔子曰 - 子曰 - 春秋 - 对曰 - 故曰 - 何也 - 或曰 - 公曰 - 此之谓也 ...     0.091955   \n",
       "12602  测曰 - 孔子曰 - 孟子曰 - 子曰 - 春秋 - 对曰 - 公曰 - 或问 - 何也 - 次八     0.929501   \n",
       "12603   散也 - 字子从 - 戟也 - 善也 - 升也 - 取也 - 数也 - 长也 - 燕伋 - 狄黑     0.574019   \n",
       "\n",
       "       Representative_document  \n",
       "0                        False  \n",
       "1                        False  \n",
       "2                        False  \n",
       "3                        False  \n",
       "4                        False  \n",
       "...                        ...  \n",
       "12599                    False  \n",
       "12600                    False  \n",
       "12601                    False  \n",
       "12602                    False  \n",
       "12603                    False  \n",
       "\n",
       "[12604 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_document_info(data['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# Fine-tune your topic representations\n",
    "representation_model = KeyBERTInspired()\n",
    "topic_model = BERTopic(representation_model=representation_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 1.48MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 500kB/s]\n",
      "README.md: 100%|██████████| 10.7k/10.7k [00:00<00:00, 8.24MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 193kB/s]\n",
      "config.json: 100%|██████████| 612/612 [00:00<00:00, 1.95MB/s]\n",
      "model.safetensors: 100%|██████████| 90.9M/90.9M [00:06<00:00, 14.8MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 2.40MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.61MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.75MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 283kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 596kB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentences\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontain_ren\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/bertopic/_bertopic.py:316\u001b[0m, in \u001b[0;36mBERTopic.fit\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m         documents: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    277\u001b[0m         embeddings: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    278\u001b[0m         images: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         y: Union[List[\u001b[38;5;28mint\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/bertopic/_bertopic.py:433\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_representative_docs(custom_documents)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# Extract topics by calculating c-TF-IDF\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Reduce topics\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_topics:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/bertopic/_bertopic.py:3636\u001b[0m, in \u001b[0;36mBERTopic._extract_topics\u001b[0;34m(self, documents, embeddings, mappings, verbose)\u001b[0m\n\u001b[1;32m   3634\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepresentation - Extracting topics from clusters using representation models.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3635\u001b[0m documents_per_topic \u001b[38;5;241m=\u001b[39m documents\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m'\u001b[39m], as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocument\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin})\n\u001b[0;32m-> 3636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_tf_idf_, words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_tf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments_per_topic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3637\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic_representations_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_words_per_topic(words, documents)\n\u001b[1;32m   3638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_topic_vectors(documents\u001b[38;5;241m=\u001b[39mdocuments, embeddings\u001b[38;5;241m=\u001b[39membeddings, mappings\u001b[38;5;241m=\u001b[39mmappings)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/bertopic/_bertopic.py:3835\u001b[0m, in \u001b[0;36mBERTopic._c_tf_idf\u001b[0;34m(self, documents_per_topic, fit, partial_fit)\u001b[0m\n\u001b[1;32m   3833\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer_model\u001b[38;5;241m.\u001b[39mpartial_fit(documents)\u001b[38;5;241m.\u001b[39mupdate_bow(documents)\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fit:\n\u001b[0;32m-> 3835\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3836\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer_model\u001b[38;5;241m.\u001b[39mtransform(documents)\n\u001b[1;32m   3837\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1339\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m \n\u001b[1;32m   1326\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "topic_model.fit(data['sentences'], y=data['contain_ren'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "topic_model = BERTopic(ctfidf_model=ctfidf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12604"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentences\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/bertopic/_bertopic.py:316\u001b[0m, in \u001b[0;36mBERTopic.fit\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m         documents: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    277\u001b[0m         embeddings: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    278\u001b[0m         images: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         y: Union[List[\u001b[38;5;28mint\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/bertopic/_bertopic.py:433\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_representative_docs(custom_documents)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# Extract topics by calculating c-TF-IDF\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Reduce topics\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_topics:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/bertopic/_bertopic.py:3636\u001b[0m, in \u001b[0;36mBERTopic._extract_topics\u001b[0;34m(self, documents, embeddings, mappings, verbose)\u001b[0m\n\u001b[1;32m   3634\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepresentation - Extracting topics from clusters using representation models.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3635\u001b[0m documents_per_topic \u001b[38;5;241m=\u001b[39m documents\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m'\u001b[39m], as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocument\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin})\n\u001b[0;32m-> 3636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_tf_idf_, words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_tf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments_per_topic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3637\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic_representations_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_words_per_topic(words, documents)\n\u001b[1;32m   3638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_topic_vectors(documents\u001b[38;5;241m=\u001b[39mdocuments, embeddings\u001b[38;5;241m=\u001b[39membeddings, mappings\u001b[38;5;241m=\u001b[39mmappings)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/bertopic/_bertopic.py:3835\u001b[0m, in \u001b[0;36mBERTopic._c_tf_idf\u001b[0;34m(self, documents_per_topic, fit, partial_fit)\u001b[0m\n\u001b[1;32m   3833\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer_model\u001b[38;5;241m.\u001b[39mpartial_fit(documents)\u001b[38;5;241m.\u001b[39mupdate_bow(documents)\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fit:\n\u001b[0;32m-> 3835\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3836\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer_model\u001b[38;5;241m.\u001b[39mtransform(documents)\n\u001b[1;32m   3837\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1339\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m \n\u001b[1;32m   1326\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mnlpa1/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "topic_model.fit(data['sentences'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlpa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
